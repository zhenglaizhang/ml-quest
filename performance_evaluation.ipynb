{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The best way to evaluate the performance of an algorithm would be to make predictions for new data to which you already know the answers. \n",
    "- Use **resampling methods** that allow you to make accurate estimates for how well your algorithm will perform on new data.\n",
    "- **overfitting**\n",
    "- We must evaluate our machine learning algorithms on data that is not used to train the algorithm.\n",
    "- The evaluation is an estimate that we can use to talk about how well we think the algorithm may actually do in practice. It is not a guarantee of performance.\n",
    "- Once we estimate the performance of our algorithm, we can then re-train the final algorithm on the entire training dataset and get it ready for operational use.\n",
    "\n",
    "\n",
    "## Four different techniques\n",
    "\n",
    "- Train and Test Sets.\n",
    "  - The size of the split can depend on the size and specifics of your dataset, although it is common to use 67% of the data for training and the remaining 33% for testing.\n",
    "  - This algorithm evaluation technique is very fast. It is ideal for large datasets (millions of records) where there is strong evidence that both splits of the data are representative of the underlying problem. Because of the speed, it is useful to use this approach when the algorithm you are investigating is slow to train.\n",
    "  - Downside: it can have a high variance, differences in the training and test dataset can result in meaningful differences in the estimate of accuracy\n",
    "- K-fold Cross Validation.\n",
    "  - Cross validation is an approach that you can use to estimate the performance of a machine learning algorithm with less variance than a single train-test set split.\n",
    "  - It works by splitting the dataset into k-parts (e.g. k=5 or k=10). Each split of the data is called a **fold**. The algorithm is trained on k-1 folds with one held back and tested on the held back fold. This is repeated so that each fold of the dataset is given a chance to be the held back test set.\n",
    "  - After running cross validation you end up with k different performance scores that you can summarize using a mean and a standard deviation.\n",
    "  - The choice of k must allow the size of each test partition to be large enough to be a reasonable sample of the problem, whilst allowing enough repetitions of the train-test evaluation of the algorithm to provide a fair estimate of the algorithms performance on unseen data. For modest sized datasets in the thousands or tens of thousands of records, k values of 3, 5 and 10 are common.\n",
    "- Leave One Out Cross Validation.\n",
    "  - You can configure cross validation so that the size of the fold is 1 (k is set to the number of observations in your dataset). This variation of cross validation is called leave-one-out cross validation.\n",
    "  - The result is a large number of performance measures that can be summarized in an effort to give a more reasonable estimate of the accuracy of your model on unseen data. A downside is that it can be a computationally more expensive procedure than k-fold cross validation.\n",
    "  - LeaveOneOut() is equivalent to KFold(n_splits=n) and LeavePOut(p=1) where n is the number of samples.\n",
    "  - Due to the high number of test sets (which is the same as the number of samples) this cross-validation method can be very costly.\n",
    "- Repeated Random Test-Train Splits.\n",
    "  - Another variation on k-fold cross validation is to create a random split of the data like the train/test split described above, but repeat the process of splitting and evaluation of the algorithm multiple times, like cross validation.\n",
    "  - This has the speed of using a train/test split and the reduction in variance in the estimated performance of k-fold cross validation. You can also repeat the process many more times as need. A down side is that repetitions may include much of the same data in the train or the test split from run to run, introducing redundancy into the evaluation.\n",
    "\n",
    "## What Techniques to Use When\n",
    "\n",
    "- Generally k-fold cross validation is the gold-standard for evaluating the performance of a machine learning algorithm on unseen data with k set to 3, 5, or 10.\n",
    "- Using a train/test split is good for speed when using a slow algorithm and produces performance estimates with lower bias when using large datasets.\n",
    "- Techniques like leave-one-out cross validation and repeated random splits can be useful intermediates when trying to balance variance in the estimated performance, model training speed and dataset size.\n",
    "- The best advice is to experiment and find a technique for your problem that is fast and produces reasonable estimates of performance that you can use to make decisions. If in doubt, use 10-fold cross validation.\n",
    "\n",
    "\n",
    "## Validation Set\n",
    "\n",
    "- A validation dataset is a sample of data held back from training your model that is used to give an estimate of model skill while tuning model’s hyperparameters.\n",
    "- The validation dataset is different from the test dataset that is also held back from the training of the model, but is instead used to give an unbiased estimate of the skill of the final tuned model when comparing or selecting between final models.\n",
    "\n",
    "## Definitions of Train, Validation, and Test Datasets\n",
    "\n",
    "- **Training Dataset**: The sample of data used to fit the model.\n",
    "- **Validation Dataset**: The sample of data used to provide an unbiased evaluation of a model fit on the training dataset while tuning model hyperparameters. The evaluation becomes more biased as skill on the validation dataset is incorporated into the model configuration.\n",
    "- **Test Dataset**: The sample of data used to provide an unbiased evaluation of a final model fit on the training dataset.\n",
    "\n",
    "```python\n",
    "# split data\n",
    "data = ...\n",
    "train, validation, test = split(data)\n",
    "\n",
    "# tune model hyperparameters\n",
    "parameters = ...\n",
    "for params in parameters:\n",
    "    model = fit(train, params)\n",
    "    skill = evaluate(model, validation)\n",
    "\n",
    "# evaludate final model for comparision with other models\n",
    "model = fit(train)\n",
    "skill = evaluate(model, test)\n",
    "```\n",
    "\n",
    "- The validation dataset may also play a role in other forms of model preparation, such as feature selection.\n",
    "- The final model could be fit on the aggregate of the training and validation datasets.\n",
    "\n",
    "\n",
    "- That there is clear precedent for what “training dataset,” “validation dataset,” and “test dataset” refer to when evaluating models.\n",
    "- That the “validation dataset” is predominately used to describe the evaluation of models when tuning hyperparameters and data preparation, and the “test dataset” is predominately used to describe the evaluation of a final tuned model when comparing it to other final models.\n",
    "- That the notions of “validation dataset” and “test dataset” may disappear when adopting alternate resampling methods like k-fold cross validation, especially when the resampling methods are nested.\n",
    "\n",
    "## Validation and Test Datasets Disappear\n",
    "\n",
    "Reference to a “validation dataset” disappears if the practitioner is choosing to tune model hyperparameters using k-fold cross-validation with the training dataset.\n",
    "\n",
    "```python\n",
    "# split data\n",
    "data = ...\n",
    "train, test = split(data)\n",
    " \n",
    "# tune model hyperparameters\n",
    "parameters = ...\n",
    "k = ...\n",
    "for params in parameters:\n",
    "\tskills = list()\n",
    "\tfor i in k:\n",
    "\t\tfold_train, fold_val = cv_split(i, k, train)\n",
    "\t\tmodel = fit(fold_train, params)\n",
    "\t\tskill_estimate = evaluate(model, fold_val)\n",
    "\t\tskills.append(skill_estimate)\n",
    "\tskill = summarize(skills)\n",
    " \n",
    "# evaluate final model for comparison with other models\n",
    "model = fit(train)\n",
    "skill = evaluate(model, test)\n",
    "```\n",
    "\n",
    "Reference to the “test dataset” too may disappear if the cross-validation of model hyperparameters using the training dataset is nested within a broader cross-validation of the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How Much Training Data\n",
    "It is important to know why you are asking about the required size of the training dataset.\n",
    "\n",
    "- **Do you have too much data?** \n",
    "  - Consider developing some learning curves to find out just how big a representative sample is (below). \n",
    "  - consider using a big data framework in order to use all available data.\n",
    "- **Do you have too little data?**\n",
    "  - Consider confirming that you indeed have too little data. Consider collecting more data, \n",
    "  - Using data augmentation methods to artificially increase your sample size.\n",
    "- **Have you not collected data yet?** \n",
    "  - Consider collecting some data and evaluating whether it is enough. \n",
    "  - if it is for a study or data collection is expensive, consider talking to a domain expert and a statistician.\n",
    "\n",
    "\n",
    "##  It Depends; No One Can Tell You\n",
    "\n",
    "\n",
    "- The complexity of the problem, nominally the unknown underlying function that best relates your input variables to the output variable.\n",
    "- The complexity of the learning algorithm, nominally the algorithm used to inductively learn the unknown underlying mapping function from specific examples.\n",
    "\n",
    "## Reason by Analogy\n",
    "- Perhaps you can look at studies on problems similar to yours as an estimate for the amount of data that may be required.\n",
    "- It is common to perform studies on how algorithm performance scales with dataset size\n",
    "\n",
    "## Use Domain Expertise\n",
    "- You need a sample of data from your problem that is representative of the problem you are trying to solve.\n",
    "- In general, the examples must be independent and identically distributed.\n",
    "- This means that there needs to be enough data to reasonably capture the relationships that may exist both between input features and between input features and output features.\n",
    "\n",
    "## Use a Statistical Heuristic\n",
    "- Factor of the number of classes\n",
    "- Factor of the number of input features\n",
    "- Factor of the number of model parameters\n",
    "\n",
    "\n",
    "## Nonlinear Algorithms Need More Data\n",
    "- These algorithms are often more flexible and even nonparametric (they can figure out how many parameters are required to model your problem in addition to the values of those parameters). They are also high-variance, meaning predictions vary based on the specific data used to train them. This added flexibility and power comes at the cost of requiring more training data, often a lot more data.\n",
    "- In fact, some nonlinear algorithms like deep learning methods can continue to improve in skill as you give them more data.\n",
    "- If a linear algorithm achieves good performance with hundreds of examples per class, you may need thousands of examples per class for a nonlinear algorithm, like random forest, or an artificial neural network.\n",
    "\n",
    "## Evaluate Dataset Size vs Model Skill\n",
    "- Design a study that evaluates model skill versus the size of the training dataset.\n",
    "- **Learning Curve**: Plotting the result as a line plot with training dataset size on the x-axis and model skill on the y-axis will give you an idea of how the size of the data affects the skill of the model on your specific problem.\n",
    "\n",
    "## Naive Guesstimate\n",
    "\n",
    "- Get and use as much data as you can.\n",
    "- You need thousands of examples.\n",
    "- No fewer than hundreds.\n",
    "- Ideally, tens or hundreds of thousands for “average” modeling problems.\n",
    "- Millions or tens-of-millions for “hard” problems like those tackled by deep learning.\n",
    "\n",
    "##  Get More Data (No Matter What!?)\n",
    "\n",
    "- Keep in mind that machine learning is a process of induction. The model can only capture what it has seen. If your training data does not include edge cases, they will very likely not be supported by the model.\n",
    "\n",
    "## Don’t Procrastinate; Get Started\n",
    "- Get all the data you can, use what you have, and see how effective models are on your problem.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "data_frame = pandas.read_csv(url, names=names)\n",
    "data = data_frame.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 76.772%\n"
     ]
    }
   ],
   "source": [
    "# Train & test sets\n",
    "X = data[:, 0:8] # shape (768, 8)\n",
    "Y = data[:, 8]\n",
    "test_size = 0.33\n",
    "# Because the split of the data is random, we want to ensure that the results are reproducible. \n",
    "# By specifying the random seed we ensure that we get the same random numbers each time we run the code.\n",
    "seed = 42\n",
    "X_train, X_test, Y_train, Y_test = model_selection.train_test_split(X, Y, test_size=test_size, random_state=seed)\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, Y_train)\n",
    "result = model.score(X_test, Y_test)\n",
    "print(\"Accuracy: %.3f%%\" % (result * 100.0))\n",
    "# the estimated accuracy for the model was approximately 76%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 76.951% (min: 70.130%, max: 85.714%, std:4.841%)\n"
     ]
    }
   ],
   "source": [
    "# K-fold cross validation\n",
    "num_instances = len(X)\n",
    "seed = 42\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "model = LogisticRegression()\n",
    "results = model_selection.cross_val_score(model, X, Y, cv=kfold)\n",
    "\n",
    "# , it is a good practice to summarize the distribution of the measures, \n",
    "# in this case assuming a Gaussian distribution of performance (a very reasonable assumption) and recording the mean and standard deviation.\n",
    "print('Accuracy: %.3f%% (min: %.3f%%, max: %.3f%%, std:%.3f%%)' % (results.mean() * 100.0, results.min() * 100.0, results.max() * 100.0, results.std() * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 76.823% (min: 0.000%, max: 100.000%, std:42.196%)\n"
     ]
    }
   ],
   "source": [
    "# Leave One Out cross validation\n",
    "loocv = model_selection.LeaveOneOut()\n",
    "model = LogisticRegression()\n",
    "results = model_selection.cross_val_score(model, X, Y, cv=loocv)\n",
    "print('Accuracy: %.3f%% (min: %.3f%%, max: %.3f%%, std:%.3f%%)' % (results.mean() * 100.0, results.min() * 100.0, results.max() * 100.0, results.std() * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 77.953% (min: 73.228%, max: 81.890%, std:2.728%)\n"
     ]
    }
   ],
   "source": [
    "# Shuffle Split cross validation\n",
    "test_size = 0.33\n",
    "seed = 42\n",
    "kfold = model_selection.ShuffleSplit(n_splits=10, test_size=test_size, random_state=seed)\n",
    "model = LogisticRegression()\n",
    "results = model_selection.cross_val_score(model, X, Y, cv=kfold)\n",
    "print('Accuracy: %.3f%% (min: %.3f%%, max: %.3f%%, std:%.3f%%)' % (results.mean() * 100.0, results.min() * 100.0, results.max() * 100.0, results.std() * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 3 4 5 6] [1]\n",
      "[1 3 4 5 6] [2]\n",
      "[1 2 4 5 6] [3]\n",
      "[1 2 3 5 6] [4]\n",
      "[1 2 3 4 6] [5]\n",
      "[1 2 3 4 5] [6]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "\n",
    "X = np.array([1, 2 ,3, 4, 5, 6])\n",
    "loo = LeaveOneOut()\n",
    "for train_idx, test_idx in loo.split(X):\n",
    "    print(X[train_idx], X[test_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n",
    "y = np.array([1, 2, 3, 4])\n",
    "rs = ShuffleSplit(n_splits=5, test_size=.25, random_state=0)\n",
    "rs.get_n_splits(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ShuffleSplit(n_splits=5, random_state=0, test_size=0.25, train_size=None)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [3 1 0] TEST: [2]\n",
      "TRAIN: [2 1 3] TEST: [0]\n",
      "TRAIN: [0 2 1] TEST: [3]\n",
      "TRAIN: [0 2 3] TEST: [1]\n",
      "TRAIN: [2 3 0] TEST: [1]\n"
     ]
    }
   ],
   "source": [
    "for train_index, test_index in rs.split(X):\n",
    "    print('TRAIN:', train_index, 'TEST:', test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
